{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPEN_API_KEY_ldw')\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt(model, system_prompt, question):\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=conversation\n",
    "    )\n",
    "\n",
    "    assistant_response = response['choices'][0]['message']['content']\n",
    "    return assistant_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m system_prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYou are a helpful historian that is factual and brief.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat was the relationship between Henry VIII and his wives? Please use a table.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m answer \u001b[39m=\u001b[39m ask_gpt(model, system_prompt, question)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(answer)\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mask_gpt\u001b[1;34m(model, system_prompt, question)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mask_gpt\u001b[39m(model, system_prompt, question):\n\u001b[0;32m      2\u001b[0m     conversation \u001b[39m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: system_prompt},\n\u001b[0;32m      4\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: question}\n\u001b[0;32m      5\u001b[0m     ]\n\u001b[1;32m----> 7\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      8\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      9\u001b[0m         messages\u001b[39m=\u001b[39;49mconversation\n\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     12\u001b[0m     assistant_response \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m assistant_response\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3.9\\envs\\lukebot\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3.9\\envs\\lukebot\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[0;32m    141\u001b[0m         timeout,\n\u001b[0;32m    142\u001b[0m         stream,\n\u001b[0;32m    143\u001b[0m         headers,\n\u001b[0;32m    144\u001b[0m         request_timeout,\n\u001b[0;32m    145\u001b[0m         typed_api_type,\n\u001b[0;32m    146\u001b[0m         requestor,\n\u001b[0;32m    147\u001b[0m         url,\n\u001b[0;32m    148\u001b[0m         params,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__prepare_create_request(\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[0;32m    153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3.9\\envs\\lukebot\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:106\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    104\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m MAX_TIMEOUT\n\u001b[1;32m--> 106\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39;49mAPIRequestor(\n\u001b[0;32m    107\u001b[0m     api_key,\n\u001b[0;32m    108\u001b[0m     api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[0;32m    109\u001b[0m     api_type\u001b[39m=\u001b[39;49mapi_type,\n\u001b[0;32m    110\u001b[0m     api_version\u001b[39m=\u001b[39;49mapi_version,\n\u001b[0;32m    111\u001b[0m     organization\u001b[39m=\u001b[39;49morganization,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    113\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    115\u001b[0m     deployment_id,\n\u001b[0;32m    116\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     params,\n\u001b[0;32m    125\u001b[0m )\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3.9\\envs\\lukebot\\lib\\site-packages\\openai\\api_requestor.py:130\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[1;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m     organization\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    128\u001b[0m ):\n\u001b[0;32m    129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_base \u001b[39m=\u001b[39m api_base \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_base\n\u001b[1;32m--> 130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m key \u001b[39mor\u001b[39;00m util\u001b[39m.\u001b[39;49mdefault_api_key()\n\u001b[0;32m    131\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_type \u001b[39m=\u001b[39m (\n\u001b[0;32m    132\u001b[0m         ApiType\u001b[39m.\u001b[39mfrom_str(api_type)\n\u001b[0;32m    133\u001b[0m         \u001b[39mif\u001b[39;00m api_type\n\u001b[0;32m    134\u001b[0m         \u001b[39melse\u001b[39;00m ApiType\u001b[39m.\u001b[39mfrom_str(openai\u001b[39m.\u001b[39mapi_type)\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_version \u001b[39m=\u001b[39m api_version \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_version\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3.9\\envs\\lukebot\\lib\\site-packages\\openai\\util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39mapi_key\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAuthenticationError(\n\u001b[0;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo API key provided. You can set your API key in code using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key = <API-KEY>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key_path = <PATH>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions."
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"gpt-4\"\n",
    "system_prompt = \"You are a helpful historian that is factual and brief.\"\n",
    "question = \"What was the relationship between Henry VIII and his wives? Please use a table.\"\n",
    "\n",
    "answer = ask_gpt(model, system_prompt, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤔 The capital of France is Paris. 🇫🇷\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "system_prompt = \"You are an AI assistant. Include emojis in your replies.\"\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "answer = ask_gpt(model, system_prompt, question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤔 I'm not sure why you need this information, but to answer your question, the average diameter of a human rectum is between 3 and 4 centimeters.\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "system_prompt = \"You are an AI assistant. Include emojis in your replies.\"\n",
    "question = \"What is the diameter of the average human rectum?\"\n",
    "\n",
    "answer = ask_gpt(model, system_prompt, question)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of New Zealand is Wellington.\n",
      "Your first question was \"What about the capital of New Zealand?\" and your second question was \"What were my last two questions?\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ask_gpt(model, system_prompt, question, conversation_history=None, max_history=3):\n",
    "    if conversation_history is None:\n",
    "        conversation_history = []\n",
    "\n",
    "    # Truncate conversation_history if it exceeds the limit\n",
    "    if len(conversation_history) > max_history:\n",
    "        conversation_history = conversation_history[-max_history:]\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        *conversation_history,\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=conversation,\n",
    "        max_tokens=256,\n",
    "        temperature=0.8,\n",
    "        top_p=1,\n",
    "    )\n",
    "\n",
    "    assistant_response = response['choices'][0]['message']['content']\n",
    "    \n",
    "    # Update the conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    return assistant_response, conversation_history\n",
    "\n",
    "# Usage example\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "question = \"What about the capital of New Zealand?\"\n",
    "\n",
    "response, convo_history = ask_gpt(model_name, system_prompt, question)\n",
    "print(response)\n",
    "\n",
    "question = \"What were my last two questions?\"\n",
    "response, convo_history = ask_gpt(model_name, system_prompt, question, conversation_history=convo_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| **Question** | **Answer** |\n",
      "|--------------|------------|\n",
      "| What about the capital of New Zealand? | The capital of New Zealand is Wellington. |\n",
      "| What were my last two questions? | Your first question was \"What about the capital of New Zealand?\" and your second question was \"What were my last two questions?\" |\n"
     ]
    }
   ],
   "source": [
    "question3 = \"Can you provide me the last two questions and the answer (verbatim) you gave, in a markdown table\"\n",
    "response3, convo_history = ask_gpt(model_name, system_prompt, question3, conversation_history=convo_history)\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
